import sys,os,math
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import concurrent.futures
from core.ask_gpt import ask_gpt
from core.prompts_storage import get_split_prompt
from difflib import SequenceMatcher
import math
from core.spacy_utils.load_nlp_model import init_nlp
from core.config_utils import load_key, get_joiner
from rich.console import Console
from rich.table import Table
import time

console = Console()

def tokenize_sentence(sentence, nlp):
    # tokenizer counts the number of words in the sentence
    doc = nlp(sentence)
    return [token.text for token in doc]

def find_split_positions(original, modified):
    split_positions = []
    parts = modified.split('[br]')
    start = 0
    whisper_language = load_key("whisper.language")
    language = load_key("whisper.detected_language") if whisper_language == 'auto' else whisper_language
    joiner = get_joiner(language)

    for i in range(len(parts) - 1):
        max_similarity = 0
        best_split = None

        for j in range(start, len(original)):
            original_left = original[start:j]
            modified_left = joiner.join(parts[i].split())

            left_similarity = SequenceMatcher(None, original_left, modified_left).ratio()

            if left_similarity > max_similarity:
                max_similarity = left_similarity
                best_split = j

        if max_similarity < 0.9:
            console.print(f"[yellow]Warning: low similarity found at the best split point: {max_similarity}[/yellow]")
        if best_split is not None:
            split_positions.append(best_split)
            start = best_split
        else:
            console.print(f"[yellow]Warning: Unable to find a suitable split point for the {i+1}th part.[/yellow]")

    return split_positions

def split_sentence(sentence: str, max_length: int, nlp, retry_attempt: int = 0) -> dict:
    """ä½¿ç”¨ LLM åˆ†å‰²å¥å­"""
    if len(sentence) <= max_length:
        return {"original": sentence, "split": sentence}
        
    # æ„å»ºæç¤ºè¯
    prompt = f"""è¯·å°†ä»¥ä¸‹å¥å­åˆ†æˆä¸¤éƒ¨åˆ†ï¼Œåœ¨æœ€åˆé€‚çš„ä½ç½®ç”¨ || åˆ†éš”ã€‚ç¡®ä¿åˆ†å‰²åçš„å¥å­è¯­ä¹‰å®Œæ•´ä¸”é•¿åº¦ç›¸è¿‘ã€‚
å¥å­ï¼š{sentence}
è¦æ±‚ï¼š
1. åªèƒ½åˆ†æˆä¸¤éƒ¨åˆ†
2. åªèƒ½ç”¨ || åˆ†éš”
3. ä¸è¦æ”¹å˜åŸæ–‡
4. ä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ ‡ç‚¹æˆ–ç©ºæ ¼
5. ç›´æ¥è¿”å›åˆ†å‰²åçš„å¥å­ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–å†…å®¹

ç¤ºä¾‹è¾“å…¥ï¼šThis is a long sentence that needs to be split into two parts with similar lengths
ç¤ºä¾‹è¾“å‡ºï¼šThis is a long sentence || that needs to be split into two parts with similar lengths"""

    try:
        response = ask_gpt(prompt)
        # ç¡®ä¿å“åº”ä¸­åŒ…å« ||
        if "||" not in response:
            raise ValueError("Response does not contain separator")
            
        # ç›´æ¥ä½¿ç”¨å“åº”æ–‡æœ¬ï¼Œä¸éœ€è¦è§£æ JSON
        return {
            "original": sentence,
            "split": response.strip()
        }
        
    except Exception as e:
        if retry_attempt < 2:  # æœ€å¤šé‡è¯•2æ¬¡
            print(f"[red]Attempt {retry_attempt + 1} failed: {str(e)}[/red]")
            print(f"[yellow]Retrying in 3 seconds...[/yellow]")
            time.sleep(3)
            return split_sentence(sentence, max_length, nlp, retry_attempt + 1)
        else:
            # å¦‚æœé‡è¯•å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„é•¿åº¦åˆ†å‰²
            mid = len(sentence) // 2
            return {
                "original": sentence,
                "split": f"{sentence[:mid]} || {sentence[mid:]}"
            }

def parallel_split_sentences(sentences: list, max_length: int, max_workers: int, nlp, retry_attempt: int = 0) -> list:
    """å¹¶è¡Œå¤„ç†å¥å­åˆ†å‰²"""
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ç§»é™¤ index å‚æ•°ï¼Œåªä¼ å…¥å¿…è¦çš„å‚æ•°
        futures = [
            executor.submit(
                split_sentence, 
                sentence, 
                max_length, 
                nlp, 
                retry_attempt
            ) for sentence in sentences
        ]
        
        results = []
        for future in futures:
            try:
                split_result = future.result()
                if "||" in split_result["split"]:
                    # å¦‚æœå¥å­è¢«åˆ†å‰²äº†ï¼Œæ·»åŠ ä¸¤ä¸ªéƒ¨åˆ†
                    parts = split_result["split"].split("||")
                    results.extend([part.strip() for part in parts])
                else:
                    # å¦‚æœå¥å­æ²¡æœ‰è¢«åˆ†å‰²ï¼Œç›´æ¥æ·»åŠ åŸå¥
                    results.append(split_result["split"].strip())
            except Exception as e:
                console.print(f"[red]Error processing sentence: {str(e)}[/red]")
                results.append(sentences[len(results)])  # å‘ç”Ÿé”™è¯¯æ—¶ä½¿ç”¨åŸå¥
                
    return results

def split_sentences_by_meaning():
    """The main function to split sentences by meaning."""
    # read input sentences
    with open('output/log/sentence_splitbynlp.txt', 'r', encoding='utf-8') as f:
        sentences = [line.strip() for line in f.readlines()]

    nlp = init_nlp()
    # ğŸ”„ process sentences multiple times to ensure all are split
    for retry_attempt in range(3):
        sentences = parallel_split_sentences(sentences, max_length=load_key("max_split_length"), max_workers=load_key("max_workers"), nlp=nlp, retry_attempt=retry_attempt)

    # ğŸ’¾ save results
    with open('output/log/sentence_splitbymeaning.txt', 'w', encoding='utf-8') as f:
        f.write('\n'.join(sentences))
    console.print('[green]âœ… All sentences have been successfully split![/green]')

if __name__ == '__main__':
    # print(split_sentence('Which makes no sense to the... average guy who always pushes the character creation slider all the way to the right.', 2, 22))
    split_sentences_by_meaning()